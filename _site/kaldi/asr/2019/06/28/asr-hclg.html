<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.0 -->
<title>kaldi中的解码图构建过程-可视化教程 | Chao Yang</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="kaldi中的解码图构建过程-可视化教程" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="注：本文翻译自 Decoding graph construction in Kaldi: A visual walkthrough" />
<meta property="og:description" content="注：本文翻译自 Decoding graph construction in Kaldi: A visual walkthrough" />
<link rel="canonical" href="http://localhost:4000/kaldi/asr/2019/06/28/asr-hclg.html" />
<meta property="og:url" content="http://localhost:4000/kaldi/asr/2019/06/28/asr-hclg.html" />
<meta property="og:site_name" content="Chao Yang" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-06-28T11:11:59+08:00" />
<script type="application/ld+json">
{"description":"注：本文翻译自 Decoding graph construction in Kaldi: A visual walkthrough","@type":"BlogPosting","url":"http://localhost:4000/kaldi/asr/2019/06/28/asr-hclg.html","headline":"kaldi中的解码图构建过程-可视化教程","dateModified":"2019-06-28T11:11:59+08:00","datePublished":"2019-06-28T11:11:59+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/kaldi/asr/2019/06/28/asr-hclg.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Chao Yang" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Chao Yang</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">kaldi中的解码图构建过程-可视化教程</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2019-06-28T11:11:59+08:00" itemprop="datePublished">Jun 28, 2019
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><em>注：本文翻译自</em> <a href="https://vpanayotov.blogspot.com/2012/06/kaldi-decoding-graph-construction.html">Decoding graph construction in Kaldi: 
A visual walkthrough</a></p>

<p>专有名词</p>
<ul>
  <li>grammar(G)</li>
  <li>lexicon(L)</li>
  <li>context-（C）</li>
  <li>HMM(H)</li>
  <li>backoff</li>
  <li>slef-loop</li>
  <li>bigram</li>
  <li>recipe</li>
</ul>

<p>最近我在使用kaldi时，识别错误率(WER)超过了40%，远高于我用的语言模型和声学模型应该得到的错误率。经过一番折腾，终于找到了原因 – 我没有在lexicon fst(L)里加上自跳转(self-loop)。</p>

<p>在kaldi中为了使得grammar fst(G)是determinizable的，使用了一个特殊的’#0’符号，因此需要加上这个自跳转使得compose L和G时可以走G中输入是’#0’的边。(??是这样吗??). 因为忘记加self-loop,我的bigram G中的back-off边就被忽略了，使得语言模型没了backoff的能力，只能一直走训练集中见过的bigram的路径，从而导致了很高的错误率。在加上self-loop后，不做其他任何改变，WER就下降到17%。</p>

<p>这个问题让我意识到自己对解码图构建的细节理解的还不够，所以我决定花时间认真研究一下。不过大词汇量的hclg各级fst太大了，很难直观观察，我尝试过用GraphViz将解码图转为可视化图片，即使用的模型量级远小于LVCSR的规模，其占用的内存和cpu也非常巨大。另外，即使没有机器性能问题，经过优化的HCLG WFST人类也几乎不可能看懂（至少远超我的理解能力）。所以我选择构建一个非常小规模的解码图来理解整个构建过程，这也是工程和科学中常用的方法。这篇blog可以作为其他非常棒的关于HCLG解码教程的补充，包括著名的hbka.pdf(WFST的圣经) 以及Dan Povey写的非常棒的kaldi解码图构建recipe。(译注：建议读者阅读此文前先阅读wfst speech decode的相关论文以及kaldi的文档对hclg的解码有一些了解)</p>

<h3 id="基本配置">基本配置</h3>

<p>我们使用非常非常小的grammars和lexicon. 本文会使用unigram和bigram G来展示逐级构建HCLG，以帮助观察二者带来的区别更好的理解解码图的构建过程。下面是训练语言模型的语料</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>K. Cay
K. ache
Cay
</code></pre></div></div>
<p>对应的unigram语言模型是:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>\data\
ngram 1=5

\1-grams:
-0.4259687 &lt;/s&gt;
-99 &lt;s&gt;
-0.60206 Cay
-0.60206 K.
-0.9030899 ache

\end\
</code></pre></div></div>
<p>对应的Bigram model语言模型是:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>\data\
ngram 1=5
ngram 2=6
\1-grams:
-0.4259687 &lt;/s&gt;
-99 &lt;s&gt; -0.30103
-0.60206 Cay -0.2730013
-0.60206 K. -0.2730013
-0.9030899 ache -0.09691

\2-grams:
-0.60206 &lt;s&gt; Cay
-0.30103 &lt;s&gt; K.
-0.1760913 Cay &lt;/s&gt;
-0.4771213 K. Cay
-0.4771213 K. ache
-0.30103 ache &lt;/s&gt;
 
\end\
</code></pre></div></div>

<p>lexicon仅包含三个词，其中两个(Cay和K.)是同音词(homophone).</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ache ey k
Cay k ey
K. k ey
</code></pre></div></div>

<p>为了使解码图尽可能简单易懂，尤其是其中的上下文相关音素fst C不要太复杂，总共的音素(phonemes)只有两个(ey和k)。</p>

<p>本文用于生成解码图和pdf图片的脚本在这<a href="https://vpanayotov.blogspot.com/2012/06/kaldi-decoding-graph-construction.html">script</a>, 使用’mkgraphs.sh’前，你需要先配置”KALDI_ROOT”指向机器上Kaldi的安装根目录。</p>

<h3 id="语法fst的构建g">语法FST的构建(G)</h3>
<p>参考根据Kaldi中关于解码图创建的文档，下面是产生语法FST的命令，其中省略了移除OOV的步骤，因为这个例子里没有OOV(out-of-vocabulary,语言模型中的词不在lexicon里)的情况：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cat lm.arpa | \
    grep -v '&lt;s&gt; &lt;s&gt;' | \
    grep -v '&lt;/s&gt; &lt;s&gt;' | \
    grep -v '&lt;/s&gt; &lt;/s&gt;' | \
    arpa2fst - | \     (step 1)
    fstprint | \
    eps2disambig.pl |\ (step 2)
    s2eps.pl | \       (step 3)
    fstcompile --isymbols=words.txt \
      --osymbols=words.txt  \
      --keep_isymbols=false --keep_osymbols=false | \
    fstrmepsilon &gt; G.fst  (step 4)
</code></pre></div></div>

<p>最终的产生G的fst如下图:
[ADD slides]</p>

<p>下面我们一步一步来看这个脚本命令是如何处理bigram语言模型的。</p>
<h4 id="step-1">step 1</h4>
<p>首先，将语言模型中的非法的<code class="highlighter-rouge">&lt;s&gt;</code>和<code class="highlighter-rouge">&lt;/s&gt;</code>的组合移除,因为这些组合会导致G FST是non-determinizable的。(译注：why？)。然后将结果送给arpa2fst，该工具将arpa转为binary格式的FST.</p>

<p>请注意，FST中的权重是对概率的自然对数取负，而ARPA文件中的数值是对概率取以10为底的对数。我们来细致的看一下arpa2fst产生的WFST。首先这个WFST有一个开始节点，表示一句话的开始(node 0),接着有三个节点分别表示<code class="highlighter-rouge">"ache"</code>, <code class="highlighter-rouge">"Cay"</code>, <code class="highlighter-rouge">"K."</code> 这三个词(分别是nodes 6, 4 and 5），另外，有一个back-off节点(node 1)和一个终止(final)节点(node 2).我们看一下<code class="highlighter-rouge">"&lt;s&gt; ache"</code>这个bigram词串在WFST中的路径，因为在训练语料中没有这个bigram，因此会存在back-off，<code class="highlighter-rouge">"&lt;s&gt; ache"</code>对应的路径为0到3，3到1，1到6. 0到3的权重是0;3到1的权重为0.69315，对应了<code class="highlighter-rouge">"&lt;s&gt;"</code>的back-off值(−ln(10−0.30103));1到6的权重为2.0794，这是<code class="highlighter-rouge">"ache"</code> (−ln(10−0.9030899))的unigram概率.</p>

<h4 id="step-2">step 2</h4>
<p>在step 2中，eps2disambig.pl脚本会将输入中的epsilon(backoff边上)转为特殊字符#0,从而使得fst是determinizable的。words.txt里应该有这个’#0’符号。(译注:)</p>

<h4 id="step-3">step 3</h4>
<p>Step 3中将”<s>"和"</s>“符号替换成epsilons。(译注:在WFST中开始和结束这个信息不需要用符号显示表达，WFST本身就蕴含了这个信息，即从开始节点出发和到达final节点)</p>

<h4 id="step-4">step 4</h4>
<p>Step 4 移除epsilon以简化FST(为何能移除？怎么移除？).</p>

<h4 id="word-symbol">word symbol</h4>
<p>WFST的符号表如下</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;eps&gt; 0
&lt;/s&gt; 1
&lt;s&gt; 2
Cay 3
K. 4
ache 5
#0 6
</code></pre></div></div>

<p>类似的对于unigram，其G FST如下，我们就不再单独分析。
[ADD slides]</p>

<p>由于HCLG的构建过程不受G的阶数的影响，为了HCLG的层次FST更简单易读，本文接下都使用unigram的G来展示HCLG的构建过程。如果你想了解bigram G构建出HCLG的过程，可以参考附件中的.pdf文件。</p>

<h3 id="词典lexiconfst的构建-l">词典(Lexicon)FST的构建 (L)</h3>

<p>Kaldi中，lexicon FST 的准备过程是相对标准的.首先使用脚本<code class="highlighter-rouge">add_lex_disambig.pl</code>为每个同音字(这个例子里是”Cay” 和 “K.”)，再发音之后添加一个单独的辅助符号进行区分，从而词典变成了:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ache ey k
Cay k ey #1
K. k ey #2
</code></pre></div></div>

<p>The L FST is produced by the make_lexicon_fst.pl script. Which takes four parameters: the lexicon file with disambiguation symbols the probability of the optional silence phones the symbol used to represent the silence phones and the disambiguation symbol for silence.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>make_lexicon_fst.pl lexicon_disambig.txt 0.5 sil '#'$ndisambig | \
   fstcompile --isymbols=lexgraphs/phones_disambig.txt \
    --osymbols=lmgraphs/words.txt \
    --keep_isymbols=false --keep_osymbols=false |\
   fstaddselfloops  $phone_disambig_symbol $word_disambig_symbol | \
   fstarcsort --sort_type=olabel \
   &gt; lexgraphs/L_disambig.fst
</code></pre></div></div>

<p>The resulting graphs can be seen on the slides below:</p>

<p>[ADD slides]</p>

<p>The graph on the first slide is the graph created by make_lexicon_fst.pl script. It adds optional silence (followed by the special silence disambiguation symbol #3) at the beginning of the sentence and also after lexicon word. On the second slide the special #0 self-loop can be seen, which is needed to pass the special symbol/word from G when it’s composed with L.</p>

<p>The phone symbol table with disambiguation symbols is:
show source
There are gaps in the IDs, because I am using a real acoustic model when building the H transducer (see below), so I wanted the phone IDs to match those from this acoustic model.</p>

<p>L*G composition</p>

<p>The commands implementing the composition are:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fsttablecompose L_disambig.fst G.fst |\
  fstdeterminizestar --use-log=true | \
  fstminimizeencoded  &gt; LG.fst
</code></pre></div></div>

<p>The commands used implement slightly different versions of the standard FST algorithms.
[ADD slides]</p>

<p>Context-dependency transducer (C)</p>

<p>The C graph is normally not created explicitly in Kaldi. Instead the fstcomposecontext tool is used to create the graph on-demand when composing with LG. Here, however we will show an explicitly created C graph for didactic purposes.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fstmakecontextfst \
 --read-disambig-syms=disambig_phones.list \\
 --write-disambig-syms=disambig_ilabels.list \
 $phones $subseq_sym ilabels |\
 fstarcsort --sort_type=olabel &gt; C.fst
</code></pre></div></div>

<p>The context dependency related graphs are given below:</p>

<p>[ADD slides]</p>

<p>The first slide shows the C transducer, created by the command above. Each state has self-loops for all auxiliary symbols introduced in L. The input symbols of the C graph are triphone IDs, which are specified by using a Kaldi-specific data structure called ilabel_info(frankly clabel_info would have been more intuitive name for me, but perhaps there is reason it’s called that way). Basically this is an array of arrays, where the the indices of the first dimension are the triphone IDs and the individual entries of the nested arrays are the IDs of the context-independent phones, which constitute the context window for the particular triphone. For example if there are triphone “a/b/c”(i.e. central phone “b” with left context “a” and right context “c”) with id “10” the eleventh entry in the ilabel_info will be an array containing the context-independent ID’s of the phones “a”, “b” and “c”. As explained in Kaldi’s documentation for context independent phones like “sil” there is a single ID in the respective ilabel_info entry. Also for convenience the IDs of the special “#X” symbols are negated, epsilon is represented by an empty array, and “#-1”(see below) with array containing a single entry with value 0. There are couple of special symbols used in the graph. The “#-1” symbol is used as input symbol at the outgoing arcs of the start (0) node. In the standard recipe described in the Mohri et al. paper mentioned above uses epsilon at this place, but the Kaldi’s docs say this would have lead to non-determinizable graphs if there are words with empty pronunciations in the lexicon. The second special symbol is “$”, which is used as output symbol for the inbound arcs of the final states. It is beneficial if C is made output-deterministic, because this leads to more efficient composition with LG. So in order to achieve this output determinism, the output(context-independent) symbols appear ahead (by N-P-1 positions in general) in respect to the input(context-dependent) symbols. So we “run out” of output symbols when we still have (N-P-1), or 1 in the most common triphone case, input symbols to flush. This is exactly the purpose of the “$” - in effect it is sort of a placeholder to be used as output symbol at the arcs entering the final(end-of-utterance) state of C. We could use epsilon instead “$” but this would have lead to inefficient composition because dead-end paths would have been explored for each triphone, not at the end of an utterance.</p>

<p>The additional “$” symbols however should be accounted for (“consumed”) in some way when C is composed with LG. The Kaldi’s tool fstaddsubsequentialloop links a special new final state with “$:ϵ” self-loop to each final state in LG as you can see on the second slide. The self-loops are added for convenience because there are in general (N-P-1) “$”-s to be consumed.</p>

<p>The third slide shows a transducer that can be composed with CLG in order to optimize the cascade. It does this by mapping all triphones, corresponding to the same HMM model to the triphone ID of a randomly chosen member of such set. For example it is not surprising that all instances of “sil” in all possible triphone context are mapped to the same triphone ID, because “sil” are context-independent and are represented by the same HMM model. Another such example is “<eps>/ey/<eps>:ey/ey/<eps>", i.e. "ey" with left context "ey" and no right context(end of utterance) is mapped to "ey" with both left and right context equal to "<eps>"(effectively meaning an utterance with a single "ey" phone). To see why this is so you can use Kaldi's draw-tree tool, which will show you that the PDFs in the HMMs for "<eps>/ey/<eps>" and "ey/ey/<eps>" are the same.</eps></eps></eps></eps></eps></eps></eps></p>

<p>CLG cascade</p>

<p>The first slide below show the unoptimized version of (unigram) CLG, and the second the graph with physical-to-logical triphone FST(see above) composed from the left.</p>

<p>[ADD slides]</p>

<p>As a result of the physical-to-logical mapping, the states in the unigram CLG cascade were reduced from 24 to 17 and the arcs from 38 to 26.</p>

<p>H transducer</p>

<p>The H graph maps from transition-ids to context-dependent phones. A transition ID uniquely identifies a phone, PDF a node and an arc within a context-dependent phone. H transducers in fact look very similar to L transducers.</p>

<p>[ADD slides]</p>

<p>There is a start/end state with an arc going into a HMM state chain - one for each context dependent physical phone(note that there is not a chain with “ey/ey/<eps>" as an output symbol for example). Also there are self-loops for each auxiliary symbol used on C level(e.g. "#-1:#-1").
The input labels for this WFST are created with a simple tool I wrote previously. They encode the information contained withing a transition-id as four underscore-separated fields - phone, HMM state index withing the triphone, PDF ID and the index of outgoing arc from the node in the second field (in this order). For example "k_1_739_1" in this notation means that this is the transition-id associated with the state "1"(i.e. the second state) of the monophone "k" having PDF ID of 739(this is in general different for the different context-dependent versions of "k") and the outgoing arc from this HMM state, which has ID "1". 
Note that the HMM-self loops are not included(thus the graph is actually called Ha) in this transducer and are only added after the whole HCLG cascade is composed.</eps></p>

<p>HCLG cascade</p>

<p>The command used for creating the full cascade (without the HMM-level self-loops) is:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fsttablecompose Ha.fst CLG_reduced.fst | \
   fstdeterminizestar --use-log=true | \
   fstrmsymbols disambig_tstate.list | \
   fstrmepslocal  | fstminimizeencoded \
   &gt; HCLGa.fst
</code></pre></div></div>
<p>i.e. the Ha and CLG(with physical-logical mapping applied) transducers are composed, determinized, the auxiliary symbols are replaced with epsilons(after all composition/determinization steps finished they are not needed anymore) and minimized.</p>

<p>[ADD slides]</p>

<p>The input labels are again in the format described above and are produced by fstmaketidsyms.</p>

<p>The graph from the second slide is created using:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>add-self-loops --self-loop-scale=0.1 \
    --reorder=true $model &lt; HCLGa.fst &gt; HCLG.fst
</code></pre></div></div>

<p>It adds self-loops adjusting their probability using the “self-loop-scale” parameter(see Kaldi’s documentation) and also reorders the transition. This reordering makes decoding faster by avoiding calculating the same acoustic score two times(in typical Bakis left-to-right topologies) for each feature frame.</p>

<p>And basically that’s all. Happy decoding!</p>

<p>公式<script type="math/tex">\sum_{i=1}^{n} i^2</script></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">print_hi</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
  <span class="n">puts</span> <span class="s">"Hi, #{name}"</span>
<span class="n">end</span>
<span class="n">print_hi</span><span class="p">(</span><span class="s">'Tom'</span><span class="p">)</span>
<span class="c1">#=&gt; prints 'Hi, Tom' to STDOUT.</span></code></pre></figure>


  </div><a class="u-url" href="/kaldi/asr/2019/06/28/asr-hclg.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Chao Yang</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Chao Yang</li><li><a class="u-email" href="mailto:chaoyang@mobvoi.com">chaoyang@mobvoi.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/placebokkk"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">placebokkk</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>My note about learning</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
