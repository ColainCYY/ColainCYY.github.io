<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-07-03T22:49:02+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Chao Yang</title><subtitle>Note</subtitle><entry><title type="html">kaldi中的解码图构建过程-可视化教程</title><link href="http://localhost:4000/kaldi/asr/2019/06/28/asr-hclg.html" rel="alternate" type="text/html" title="kaldi中的解码图构建过程-可视化教程" /><published>2019-06-28T11:11:59+08:00</published><updated>2019-06-28T11:11:59+08:00</updated><id>http://localhost:4000/kaldi/asr/2019/06/28/asr-hclg</id><content type="html" xml:base="http://localhost:4000/kaldi/asr/2019/06/28/asr-hclg.html">&lt;p&gt;&lt;em&gt;注：本文翻译自 &lt;a href=&quot;https://vpanayotov.blogspot.com/2012/06/kaldi-decoding-graph-construction.html&quot;&gt;Decoding graph construction in Kaldi: 
A visual walkthrough&lt;/a&gt;, 并增加了一些解释。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;建议读者阅读此本文前先阅读wfst speech decode的&lt;a href=&quot;https://cs.nyu.edu/~mohri/pub/hbka.pdf&quot;&gt;论文&lt;/a&gt;以及kaldi的解码图构建&lt;a href=&quot;http://kaldi-asr.org/doc/graph_recipe_test.html&quot;&gt;文档&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;草稿，需要校审&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;专有名词&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Grammar fst(G) 语法模型，建模文本的概率，即N-gram语言模型,这切实是个FSA(acceptor),即输入和输出是一样的FST.&lt;/li&gt;
  &lt;li&gt;Lexicon fst(L) 词典模型，建模音素序列到词序列之间的关系&lt;/li&gt;
  &lt;li&gt;Context-Dependent fst(C）建模上下文相关音素序列到单音素序列的转换关系&lt;/li&gt;
  &lt;li&gt;HMM Fst(H) 建模上下文相关音素HMM中的边序列到上下文相关音素序列的转换关系。&lt;/li&gt;
  &lt;li&gt;self-loop 自跳转，fst中从当前state跳出仍回到该state的边&lt;/li&gt;
  &lt;li&gt;bi-gram 二阶语言模型，当前词的条件概率只和前一个词有关&lt;/li&gt;
  &lt;li&gt;backoff 语言模型中，对于训练集中缺失的N阶gram，使用N-1阶的概率进行估计&lt;/li&gt;
  &lt;li&gt;recipe kaldi里的完成某个任务整个可执行脚本&lt;/li&gt;
  &lt;li&gt;cd-phone 上下文相关音素&lt;/li&gt;
  &lt;li&gt;transition-id 解码fst的输入，每个transition-id对应一个(phone, HMM-state, pdf-id, transition-index)元组&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最近我在使用kaldi时，识别错误率(WER)超过了40%，远高于我用的语言模型和声学模型应该得到的错误率。经过一番折腾，终于找到了原因 – 我没有在lexicon fst(L)里加上自跳转(self-loop)。&lt;/p&gt;

&lt;p&gt;在kaldi中为了使得Grammar FST(G)是determinizable的，在back-off边上使用了一个特殊的’#0’符号(也可以使用epsilon，但是这样G就是non-determinizable)，因此需要在Lexicon FST(L)加上一个自跳转使得compose L和G时可以经过G中输入是’#0’的边. 因为忘记加这个自跳转边,我的bigram G中的back-off边在compose时就被忽略了，使得语言模型没了backoff的能力，解码图里只存在训练集中见过的bigram的路径，从而导致了很高的错误率。而加上self-loop后，不用做其他任何改变，WER就下降到17%。&lt;/p&gt;

&lt;p&gt;这个问题让我意识到自己对于解码图构建过程的细节理解的还不够，所以我决定花些时间认真研究一下。对于大词汇量的hclg而言，各级fst都太大，很难直观观察，我尝试过用GraphViz将解码图转为可视化图片，即使用的模型量级并不大（远小于LVCSR的规模），其占用的内存和cpu也非常巨大。另外，即使忽略机器性能问题，被优化过的HCLG WFST人类也几乎看不懂（至少远超我的理解能力）。所以本文我通过构建一个非常小规模的解码图来演示整个构建过程以帮助理解，这也是工程和科学中常用的方法。有一些很好的关于WFST解码的资料，包括著名的&lt;a href=&quot;https://cs.nyu.edu/~mohri/pub/hbka.pdf&quot;&gt;hbka.pdf&lt;/a&gt;(WFST的圣经) 以及Dan Povey写的非常棒的kaldi解码图构建&lt;a href=&quot;http://kaldi-asr.org/doc/graph_recipe_test.html&quot;&gt;recipe&lt;/a&gt;，这篇blog可以作为这些资料的补充。&lt;/p&gt;

&lt;h3 id=&quot;基本配置&quot;&gt;基本配置&lt;/h3&gt;

&lt;p&gt;本文使用规模很小的grammars和lexicon来演示完整HCLG构建过程. 语言模型方面，会unigram和bigram演示G FST的构建，而在逐级构建HCLG时则为了易懂使用unigram。下面是训练语言模型使用的语料&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;K. Cay
K. ache
Cay
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;对应的unigram语言模型是:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;\data\
ngram 1=5

\1-grams:
-0.4259687 &amp;lt;/s&amp;gt;
-99 &amp;lt;s&amp;gt;
-0.60206 Cay
-0.60206 K.
-0.9030899 ache

\end\
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;对应的Bigram model语言模型是:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;\data\
ngram 1=5
ngram 2=6
\1-grams:
-0.4259687 &amp;lt;/s&amp;gt;
-99 &amp;lt;s&amp;gt; -0.30103
-0.60206 Cay -0.2730013
-0.60206 K. -0.2730013
-0.9030899 ache -0.09691

\2-grams:
-0.60206 &amp;lt;s&amp;gt; Cay
-0.30103 &amp;lt;s&amp;gt; K.
-0.1760913 Cay &amp;lt;/s&amp;gt;
-0.4771213 K. Cay
-0.4771213 K. ache
-0.30103 ache &amp;lt;/s&amp;gt;
 
\end\
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;lexicon仅包含三个词，其中两个(Cay和K.)是同音词(homophone).&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ache ey k
Cay k ey
K. k ey
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;为了使解码图尽可能简单易懂，尤其是其中用于将cd-phone变换为phone的fst C不要太复杂，总共的音素(phonemes)只有两个(ey和k).&lt;/p&gt;

&lt;p&gt;本文用于生成解码图和pdf图片的脚本在这&lt;a href=&quot;https://vpanayotov.blogspot.com/2012/06/kaldi-decoding-graph-construction.html&quot;&gt;script&lt;/a&gt;,其中也包含了该文章中展示的各fst的pdf，对于一些比较大的fst，可以直接打开pdf放大看. 使用’mkgraphs.sh’前，你需要先配置”KALDI_ROOT”指向机器上Kaldi的安装根目录。&lt;/p&gt;

&lt;h3 id=&quot;语法fst的构建g&quot;&gt;语法FST的构建(G)&lt;/h3&gt;
&lt;p&gt;参考根据Kaldi中关于解码图创建的文档，下面是产生语法FST的命令，其中省略了移除OOV的步骤，因为这个例子里没有OOV(out-of-vocabulary,语言模型中的词不在lexicon里)的情况：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cat lm.arpa | \
    grep -v '&amp;lt;s&amp;gt; &amp;lt;s&amp;gt;' | \
    grep -v '&amp;lt;/s&amp;gt; &amp;lt;s&amp;gt;' | \
    grep -v '&amp;lt;/s&amp;gt; &amp;lt;/s&amp;gt;' | \
    arpa2fst - | \     (step 1)
    fstprint | \
    eps2disambig.pl |\ (step 2)
    s2eps.pl | \       (step 3)
    fstcompile --isymbols=words.txt \
      --osymbols=words.txt  \
      --keep_isymbols=false --keep_osymbols=false | \
    fstrmepsilon &amp;gt; G.fst  (step 4)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;最终的产生G的fst如下图:&lt;/p&gt;

&lt;p style=&quot;width: 640px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/assets/images/hclg/G_bi.png&quot; alt=&quot;G_bi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下面我们一步一步来看这个脚本命令是如何处理bigram语言模型的。&lt;/p&gt;
&lt;h4 id=&quot;step-1&quot;&gt;step 1&lt;/h4&gt;
&lt;p&gt;首先，将语言模型中的非法的&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;s&amp;gt;&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;/s&amp;gt;&lt;/code&gt;的组合移除,因为这些组合会导致G FST是non-determinizable的。(译注：why？)。然后将结果送给arpa2fst，该工具将arpa转为binary格式的FST.&lt;/p&gt;

&lt;p style=&quot;width: 640px;&quot; class=&quot;center&quot;&gt;请注意，FST中的权重是对概率的自然对数取负，而ARPA文件中的数值是对概率取以10为底的对数。我们来细致的看一下arpa2fst产生的WFST。首先这个WFST有一个开始节点，表示一句话的开始(node 0),接着有三个节点分别表示&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;ache&quot;&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;Cay&quot;&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;K.&quot;&lt;/code&gt; 这三个词(分别是nodes 6, 4 and 5），另外，有一个back-off节点(node 1)和一个终止(final)节点(node 2).我们看一下&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;&amp;lt;s&amp;gt; ache&quot;&lt;/code&gt;这个bigram词串在WFST中的路径，因为在训练语料中没有这个bigram，因此会存在back-off，&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;&amp;lt;s&amp;gt; ache&quot;&lt;/code&gt;对应的路径为0到3，3到1，1到6. 0到3的权重是0;3到1的权重为0.69315，对应了&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;&amp;lt;s&amp;gt;&quot;&lt;/code&gt;的back-off值(−ln(10−0.30103));1到6的权重为2.0794，这是&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;ache&quot;&lt;/code&gt; (−ln(10−0.9030899))的unigram概率.
&lt;img src=&quot;/assets/images/hclg/G_bi_raw.png&quot; alt=&quot;G_bi_raw&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;step-2&quot;&gt;step 2&lt;/h4&gt;
&lt;p style=&quot;width: 640px;&quot; class=&quot;center&quot;&gt;在step 2中，eps2disambig.pl脚本会将输入中的epsilon(backoff边上)转为特殊字符#0,从而使得fst是determinizable的。words.txt里应该有这个’#0’符号。(译注:)
&lt;img src=&quot;/assets/images/hclg/G_bi_eps2disambig.png&quot; alt=&quot;G_bi_eps2disambig&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;step-3&quot;&gt;step 3&lt;/h4&gt;
&lt;p style=&quot;width: 640px;&quot; class=&quot;center&quot;&gt;Step 3中将”&lt;s&gt;&quot;和&quot;&lt;/s&gt;“符号替换成epsilons。(译注:在WFST中开始和结束这个信息不需要用符号显示表达，WFST本身就蕴含了这个信息，即从开始节点出发和到达final节点)
&lt;img src=&quot;/assets/images/hclg/G_bi_s2eps.png&quot; alt=&quot;G_bi_s2eps&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;step-4&quot;&gt;step 4&lt;/h4&gt;
&lt;p style=&quot;width: 640px;&quot; class=&quot;center&quot;&gt;Step 4 移除epsilon以简化FST(为何能移除？怎么移除？),得到最终的G fst
&lt;img src=&quot;/assets/images/hclg/G_bi.png&quot; alt=&quot;G_bi&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;word-symbol&quot;&gt;word symbol&lt;/h4&gt;
&lt;p&gt;WFST的符号表如下&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;eps&amp;gt; 0
&amp;lt;/s&amp;gt; 1
&amp;lt;s&amp;gt; 2
Cay 3
K. 4
ache 5 
#0 6
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p style=&quot;width: 640px;&quot; class=&quot;center&quot;&gt;类似的对于unigram，其G FST如下，我们就不再单独分析。
&lt;img src=&quot;/assets/images/hclg/G_uni.png&quot; alt=&quot;G_uni&quot; /&gt;&lt;/p&gt;
&lt;p&gt;由于HCLG的构建过程不受G的阶数的影响，为了HCLG的层次FST更简单易读，本文接下都使用unigram的G来展示HCLG的构建过程。如果你想了解bigram G构建出HCLG的过程，可以参考附件中的.pdf文件。&lt;/p&gt;

&lt;h3 id=&quot;词典lexiconfst的构建-l&quot;&gt;词典(Lexicon)FST的构建 (L)&lt;/h3&gt;

&lt;p&gt;Kaldi中，lexicon FST 的准备过程是相对标准的.首先使用脚本&lt;code class=&quot;highlighter-rouge&quot;&gt;add_lex_disambig.pl&lt;/code&gt;为每个同音字(这个例子里是”Cay” 和 “K.”)，再发音之后添加一个单独的辅助符号进行区分，从而词典变成了:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ache ey k
Cay k ey #1
K. k ey #2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;make_lexicon_fst.pl用于创建L FST. 这个脚本有四个参数&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;词典文件，其中包含了disambiguation符号(本例子里为#1，#2)&lt;/li&gt;
  &lt;li&gt;可选：silence音素的概率&lt;/li&gt;
  &lt;li&gt;可选：silence音素的符号&lt;/li&gt;
  &lt;li&gt;可选：silence的disambiguation符号（解释下)&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;make_lexicon_fst.pl lexicon_disambig.txt 0.5 sil '#'$ndisambig | \
   fstcompile --isymbols=lexgraphs/phones_disambig.txt \
    --osymbols=lmgraphs/words.txt \
    --keep_isymbols=false --keep_osymbols=false |\
   fstaddselfloops $phone_disambig_symbol $word_disambig_symbol | \
   fstarcsort --sort_type=olabel \
   &amp;gt; lexgraphs/L_disambig.fst
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;make_lexicon_fst.pl 脚本会在每个词的发音开头和结束加入可选的silence音素（允许一个词的开头和结尾有silence），并且加入silence的disambiguation符号，一般为同音词已使用的最大disambiguation符号加1，本例中为#2+1=#3,只有在lexicon本身包含同音词需要disambig时才需要为silence引入一个disambiguation符号。具体解释见（？？？）&lt;/p&gt;

&lt;p style=&quot;width: 640px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/assets/images/hclg/L_disambig_without_silloop.png&quot; alt=&quot;L_disambig_without_silloop&quot; /&gt;&lt;/p&gt;

&lt;p&gt;另外，记得G中用于backoff的#0辅助符号吗，我们的词典里并没有#0这个词，因此compose时G中输入为#0（要求L中输出时#0）这个路径都被丢弃了，这里的一个trick时加入一个&lt;code class=&quot;highlighter-rouge&quot;&gt;#0:#0&lt;/code&gt;的self-loop边，:左边的#0是发音词典中引入的disambiguation符号，:右边的#0是G中的为了backoff引入的辅助符号.&lt;/p&gt;

&lt;p style=&quot;width: 640px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/assets/images/hclg/L_disambig.png&quot; alt=&quot;L_disambig&quot; /&gt;&lt;/p&gt;

&lt;p&gt;带有disambiguation符号的的phone符号表如下&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;eps&amp;gt; 0
ey 15
k 22
sil 42
#0 43
#1 44
#2 45
#3 46
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;一般phones.txt这个文件里symbol id从0开始连续的，注意这里不是0，1，2，3而是0，15，22，42，这是因为作者从一个真实声学模型的音素集里只抽取了两个音素，但是为了复用该声学模型音素集对应的H，仍保持了符号id. 另外可以看出，#x是从最大的id继续增加的.&lt;/p&gt;

&lt;h3 id=&quot;lg-composition&quot;&gt;LG composition&lt;/h3&gt;

&lt;p&gt;L和G的 composition操作如下&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fsttablecompose L_disambig.fst G.fst |\
  fstdeterminizestar --use-log=true | \
  fstminimizeencoded  &amp;gt; LG.fst
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这个命令里的&lt;code class=&quot;highlighter-rouge&quot;&gt;fsttablecompose/fstdeterminizestar/fstminimizeencoded&lt;/code&gt;均为kaldi实现的fst操作，而不是openfst的命令，这个实现和openfst的标准compose/determinize/minimize有些微小的区别. 具体区别参考 &lt;a href=&quot;http://kaldi-asr.org/doc/fst_algo.html&quot;&gt;kaldi-fst&lt;/a&gt;&lt;/p&gt;

&lt;p style=&quot;width: 640px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/assets/images/hclg/LG_uni.png&quot; alt=&quot;LG_uni&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;上下文相关音素context-dependent-phones-cd-phonesfst-c&quot;&gt;上下文相关音素(context-dependent phones, cd-phones)FST (C)&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;关于C FST以及CLG的处理设计&lt;code class=&quot;highlighter-rouge&quot;&gt;#-1&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;$&lt;/code&gt;以及&lt;code class=&quot;highlighter-rouge&quot;&gt;logical/physical&lt;/code&gt;的概念，是kaldi里理解起来比较麻烦的部分，本文仅简要介绍帮助初步理解，更深入理解需要看kaldi的文档/代码以及其他介绍&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;在Kaldi中一般不会显示创建出单独的C FST再和LG compose，而是使用fstcomposecontext工具根据LG动态的生成CLG（注：因为穷举所有的cd-phones非常浪费，根据LG中的需要动态的创建用到的&lt;code class=&quot;highlighter-rouge&quot;&gt;cd-phones&lt;/code&gt;会节省很多开销）。这里，处于演示的目的，现实的创建一个C FST。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fstmakecontextfst \
 --read-disambig-syms=disambig_phones.list \\
 --write-disambig-syms=disambig_ilabels.list \
 $phones $subseq_sym ilabels |\
 fstarcsort --sort_type=olabel &amp;gt; C.fst
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;创建的上下文相关的FST如下（C, 将&lt;code class=&quot;highlighter-rouge&quot;&gt;cd-phone&lt;/code&gt;转换为上下文无关音素的FST）&lt;/p&gt;

&lt;p style=&quot;width: 640px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/assets/images/hclg/C.png&quot; alt=&quot;C&quot; /&gt;&lt;/p&gt;

&lt;p&gt;该FST中，每个state都有一些自跳转，处理L中引入的辅助符号#X.&lt;/p&gt;

&lt;p&gt;C FST的输入符号是triphone的id（这里已经把id转换为对应的可读符号/left-ctx/phone/right-ctx）, 实际上，kaldi用一个ilabel_info数据结构来存储C fst的输入符号的信息。ilabel_info是一个成员是数组的数组，数组里的每个元素记录一个C fst的输入符号信息。&lt;/p&gt;

&lt;p&gt;举个例子，若triphone “a/b/c”在C中的symbol id是10(id从0开始)，则ilabel_info中的第11个元素存储了”a/b/c”的完整上下文信息，若a，b，c在L中的id分别为10，11，12，则该元素存储的信息为[10 11 20].&lt;/p&gt;

&lt;p&gt;ilabel_info中还会存储#X符号，但是会将其在L中的id取负，如在L中#1的id是44，则在C中的input也会有一个对应的#1符号，其存储为[-44],之所以用符号，是为了方便直接判断当前是不是辅助符号。&lt;/p&gt;

&lt;p&gt;以(N=3,P=1)为例每个边的格式为a/b/c:c，输入是a/b/c，即当前音素是b，左边上下文是a，右边上下文音素是c，要注意的是，边上输出不是当前音素b，而是下一个音素c.这就带来两个问题：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;假设某个词发音是a b c,对于词的第一个音素，a其三音素为&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;eps&amp;gt;/a/b&lt;/code&gt;,对应的边为&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;eps&amp;gt;/a/b：b&lt;/code&gt;，这时输出是b，那输出的a的边应该是什么样的，可以是&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;eps&amp;gt;/&amp;lt;eps&amp;gt;/a&lt;/code&gt;，而在kaldi中统一用#-1表示输入是空的，所以ilabel_info还存储了符号#-1，用于表示从初始状态开始先接受空输入产生音素。在start状态后会有(N-P-1)个#-1输入，才进入到第一个正常的&lt;code class=&quot;highlighter-rouge&quot;&gt;cd-phone&lt;/code&gt;输入的边。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;当前上下文音素是a/b/c时输出了c，但是如果到了句子末尾，接下来的当前音素是&lt;code class=&quot;highlighter-rouge&quot;&gt;b/c/&amp;lt;eps&amp;gt;&lt;/code&gt;应该输出什么呢？可以输出&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;eps&amp;gt;&lt;/code&gt;,但kaldi为C FST引入了一个专用符号$，从而使的C是output deterministic的，在和LG compose时更加高效。在final状态前会有(N-P-1)个$输出。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;既然在C中加入了$这个输出，LG中也需要加入输入为$的边。使用Kaldi的工具fstaddsubsequentialloop为LG进行修改。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fstaddsubsequentialloop ${subseq_sym} cascade/LG_uni.fst &amp;gt; cdgraphs/LG_uni_subseq.fst
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;可以看到，每个LG中的final状态用”$:ϵ”边连接到一个带self-loop边的新的final状态上。有self-loop是为了处理C中连续(N-P-1)个”$”输出。&lt;/p&gt;

&lt;p style=&quot;width: 640px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/assets/images/hclg/LG_uni_subseq.png&quot; alt=&quot;LG_uni_subseq&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;clg-cascade&quot;&gt;CLG cascade&lt;/h3&gt;

&lt;p&gt;C/LG compose的脚本如下，过程和L/G compose 一样&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fsttablecompose cdgraphs/C.fst cdgraphs/LG_uni_subseq.fst |\
  fstdeterminizestar --use-log=true |\
  fstminimizeencoded \
  &amp;gt; cascade/CLG_uni.fst
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;得到的CLG fst如下&lt;/p&gt;

&lt;p style=&quot;width: 640px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/assets/images/hclg/CLG_uni.png&quot; alt=&quot;CLG_uni&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于CLG FST，我们可以使用一个FST将输入&lt;code class=&quot;highlighter-rouge&quot;&gt;cd-phone&lt;/code&gt;进一步优化，减小&lt;code class=&quot;highlighter-rouge&quot;&gt;cd-phone&lt;/code&gt;的个数。&lt;/p&gt;

&lt;p&gt;注意，kaldi为了减小&lt;code class=&quot;highlighter-rouge&quot;&gt;cd-phone&lt;/code&gt;的个数，一般使用决策树绑定，kaldi的决策数绑定支持将HMM中arc或者state的绑定，我们只讨论arc的绑定，绑定后的arc称为senon。加入某些&lt;code class=&quot;highlighter-rouge&quot;&gt;cd-phone&lt;/code&gt; topo内对应的arc都绑定到同样的senone上，这些&lt;code class=&quot;highlighter-rouge&quot;&gt;cd-phone&lt;/code&gt;（logical &lt;code class=&quot;highlighter-rouge&quot;&gt;cd-phone&lt;/code&gt;）对应的同一个physical &lt;code class=&quot;highlighter-rouge&quot;&gt;cd-phone&lt;/code&gt;，kaldi会随便在同一组logical &lt;code class=&quot;highlighter-rouge&quot;&gt;cd-phone&lt;/code&gt;中选一个作为其phsical &lt;code class=&quot;highlighter-rouge&quot;&gt;cd-phone&lt;/code&gt;的id。目前CLG中的输入可以认为是logical &lt;code class=&quot;highlighter-rouge&quot;&gt;cd-phone&lt;/code&gt;，即所有可能组合的音素，根据决策树绑定，进一步减小输入。&lt;/p&gt;

&lt;p&gt;可以构建一个fst，将physical&lt;code class=&quot;highlighter-rouge&quot;&gt;cd-phone&lt;/code&gt;映射为logical&lt;code class=&quot;highlighter-rouge&quot;&gt;cd-phone&lt;/code&gt;，&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;make-ilabel-transducer --write-disambig-syms=cdgraphs/disambig_ilabels_remapped.list\
 cdgraphs/ilabels $tree $model cdgraphs/ilabels.remapped \
 &amp;gt; cdgraphs/ilabel_map.fst
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ilabel_map.fst是physical到logical的映射，如下图&lt;/p&gt;

&lt;p style=&quot;width: 640px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/assets/images/hclg/ilabel_map.png&quot; alt=&quot;ilabel_map&quot; /&gt;&lt;/p&gt;

&lt;p&gt;将该fst和CLG fst compose，即可得到减小后的输入为physical &lt;code class=&quot;highlighter-rouge&quot;&gt;cd-phone&lt;/code&gt;的CLG了。通过增加这个physical到logical的映射，unigram CLG的状态从24降到17，边从38降到26&lt;/p&gt;

&lt;p style=&quot;width: 640px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/assets/images/hclg/CLG_uni_reduced.png&quot; alt=&quot;CLG_uni_reduced&quot; /&gt;&lt;/p&gt;

&lt;p&gt;kaldi中，把sil当作是上下文无关的音素，所有的&lt;code class=&quot;highlighter-rouge&quot;&gt;x/sil/y&lt;/code&gt;都绑定到同一个&lt;code class=&quot;highlighter-rouge&quot;&gt;physical cd-phone&lt;/code&gt;上，可以在该fst里看到这个映射。另外，我们的例子中，有一条边是&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;eps&amp;gt;/ey/&amp;lt;eps&amp;gt;:ey/ey/&amp;lt;eps&amp;gt;&lt;/code&gt;，表示对于音素’ey’,若其左边是’ey’,右边是&lt;code class=&quot;highlighter-rouge&quot;&gt;'&amp;lt;eps&amp;gt;'&lt;/code&gt;,等价于左边是&lt;code class=&quot;highlighter-rouge&quot;&gt;'&amp;lt;eps&amp;gt;'&lt;/code&gt;右边是&lt;code class=&quot;highlighter-rouge&quot;&gt;'&amp;lt;eps&amp;gt;'&lt;/code&gt;.你可以用kaldi的draw-tree工具把tree中的信息输出，&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;&amp;lt;eps&amp;gt;/ey/&amp;lt;eps&amp;gt;&quot;&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;ey/ey/&amp;lt;eps&amp;gt;&quot;&lt;/code&gt;的HMM中的PDF是一样的。&lt;/p&gt;

&lt;h3 id=&quot;h-fst&quot;&gt;H FST&lt;/h3&gt;

&lt;p&gt;H fst的功能是把&lt;code class=&quot;highlighter-rouge&quot;&gt;transition-id&lt;/code&gt;序列映射到&lt;code class=&quot;highlighter-rouge&quot;&gt;cd-phone&lt;/code&gt;序列. kaidi中，&lt;code class=&quot;highlighter-rouge&quot;&gt;一个cd-phone的当前phone，cd-phone的一个state，state上的一条边，以及其对应的pdf&lt;/code&gt;有一个唯一标识&lt;code class=&quot;highlighter-rouge&quot;&gt;transition-id&lt;/code&gt;.最终解码fst的输入不是pdf-id而是&lt;code class=&quot;highlighter-rouge&quot;&gt;transition-id&lt;/code&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;H fst&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;L fst&lt;/code&gt;看起来很类似.&lt;/p&gt;

&lt;p style=&quot;width: 640px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/assets/images/hclg/Ha.png&quot; alt=&quot;Ha&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这个fst从一个start节点（同时也是final节点，这个fst是个closure）开始，进入到各个cd-phone的HMM。每个physical cd-phone输出对应一个hmm state的序列的输入（kaldi中其实是transition id序列）。注意并没有一个输出是&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;ey/ey/&amp;lt;eps&amp;gt;&quot;&lt;/code&gt;的序列，因为&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;ey/ey/&amp;lt;eps&amp;gt;&quot;&lt;/code&gt;是一个logical cd-phone，其physical cd-phone 是&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;&amp;lt;eps&amp;gt;/ey/&amp;lt;eps&amp;gt;&quot;&lt;/code&gt;. 另外，start节点上还有一些自跳转用于处理C fst里引入的辅助符号，如&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;#-1:#-1&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;这个FST的输入标签是我用一个自己写的工具&lt;a href=&quot;https://bitbucket.org/vdp/kaldi-rm1-mod/src/4fb1791d1210/cxx/fstmaketidsyms/fstmaketidsyms.cc&quot;&gt;fstmaketidsyms&lt;/a&gt;打印出来，这个标签展示了一个transition-id包含的四个信息（下划线分割），包括对应的cd-phone的对应的phone，cd-phone的中的HMM state，pdf的id以及该state上的对应的出度边的在该state上的index. 例如”k_1_739_1”表示该transition-id对应了音素k拓扑的一个state上的第1个出度边，其绑定到的pdf-id是739.（音素k的不同的cd-phone，同样位置的边上的pdf可能会绑定到不同的pdf上）
注意这个fst里没有包含hmm的自跳转，所以这个是Ha fst而不是H fst.当HCLG compose完了在加入HMM-self.&lt;/p&gt;

&lt;h3 id=&quot;hclg-fst&quot;&gt;HCLG fst&lt;/h3&gt;

&lt;p&gt;下面命令用于创建完整的HCLG（不包含HMM中的状态上的自跳转边）&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fsttablecompose Ha.fst CLG_reduced.fst | \
   fstdeterminizestar --use-log=true | \
   fstrmsymbols disambig_tstate.list | \
   fstrmepslocal  | fstminimizeencoded \
   &amp;gt; HCLGa.fst
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Ha fst和CLG fst(加入了physical到logical的mapping)进行compose， determinize操作， 然后将辅助符号(#n,compose和determinze之后这些辅助符号就没用了)用epsilons替代，然后再做minimize.&lt;/p&gt;

&lt;p&gt;下图是输出结果，其中输入符号和上面Ha的产生方式一样，也是用&lt;a href=&quot;https://bitbucket.org/vdp/kaldi-rm1-mod/src/4fb1791d1210/cxx/fstmaketidsyms/fstmaketidsyms.cc&quot;&gt;fstmaketidsyms&lt;/a&gt;工具产生。&lt;/p&gt;

&lt;p style=&quot;width: 640px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/assets/images/hclg/HCLGa_uni.png&quot; alt=&quot;HCLGa_uni&quot; /&gt;&lt;/p&gt;

&lt;p&gt;然后我们加上HMM里的自跳转边。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;add-self-loops --self-loop-scale=0.1 \
    --reorder=true $model &amp;lt; HCLGa.fst &amp;gt; HCLG.fst
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;add-self-loops在加入self-loop时，会根据self-loop-scale参数调整概率（细节见kaldi文档）并且会对transition重排序(reorder),这个重排序操作使得对于每一帧特征，不需要计算两次同样的声学模型得分（对于通常的Bakis从左到右的HMM拓扑），从而可以使得解码过程更快。最终HCLG的解码涂伟&lt;/p&gt;

&lt;p style=&quot;width: 640px;&quot; class=&quot;center&quot;&gt;&lt;img src=&quot;/assets/images/hclg/HCLG_uni.png&quot; alt=&quot;HCLG_uni&quot; /&gt;&lt;/p&gt;

&lt;p&gt;基本上就这些了，感谢阅读。&lt;/p&gt;</content><author><name></name></author><summary type="html">注：本文翻译自 Decoding graph construction in Kaldi: A visual walkthrough, 并增加了一些解释。</summary></entry><entry><title type="html">Normalize Flow</title><link href="http://localhost:4000/dl/2019/03/20/tts-flow.html" rel="alternate" type="text/html" title="Normalize Flow" /><published>2019-03-20T10:00:00+08:00</published><updated>2019-03-20T10:00:00+08:00</updated><id>http://localhost:4000/dl/2019/03/20/tts-flow</id><content type="html" xml:base="http://localhost:4000/dl/2019/03/20/tts-flow.html">&lt;h2 id=&quot;预备知识&quot;&gt;预备知识&lt;/h2&gt;
&lt;h3 id=&quot;雅各比阵-j&quot;&gt;雅各比阵 J&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;对于一个非线性（或线性）函数f(x)，对于其中任意一个点xi，可以认为在xi周围无穷小的区域内做了一个线性变换，每个点的线性变换都不一样（也是关于x的函数），该线性变化的矩阵的函数即是雅各比阵。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;行列式-det&quot;&gt;行列式 det&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;一个线性变化把一个单位大小（大小为1的）的空间变换成的新的空间的大小。或者说一个线性变化对原空间的放大比例。&lt;/li&gt;
  &lt;li&gt;只有方阵有行列式，因为方阵对应的线性变化输入和输出的纬度是一样的，才可以比较两个空间的体积，一个2维和3维的空间之间的体积无法比较&lt;/li&gt;
  &lt;li&gt;方阵的逆的行列式=方阵的行列式的逆， 说明一个线性变化把原空间放大多少，他对应的逆线性变化就把原空间缩小多少。如果一个线性变化A的行列式为0，即A把一个空间缩小为0，则A的逆变化是把空间放大无穷倍，这种变化不存在（不存在有限值），因此当A的行列式为0，A不可逆。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;normalize-flow&quot;&gt;Normalize Flow&lt;/h2&gt;
&lt;h3 id=&quot;问题1&quot;&gt;问题1:&lt;/h3&gt;
&lt;p&gt;x服从p(x)分布，若y=f(x),则y服从什么分布？&lt;/p&gt;

&lt;p&gt;假设y服从q(y)分布&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;p(x)*dx = q(f(x))*df(x)
q(y) = p(g(y)) |det J(g(y))|  
g=f^-1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;问题2&quot;&gt;问题2&lt;/h3&gt;
&lt;p&gt;假如y服从q(y)，我们想从q(y)采样出一系列样本y，但是q(y)不好采样，或者我们就不知道q(y)的解析形式。这么办。&lt;/p&gt;

&lt;p&gt;解决方法：
把问题1反过来看，选择一个简单的分布p(x)，如果我们能找到一个变换f，使得f(x)服从q(y).则只要从p(x)中采样，然后执行f计算即可。任务就变为选择一个p(x)和找到一个f.
寻找f的过程需要计算q(y)，因此对f有两个要求：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;f可逆&lt;/li&gt;
  &lt;li&gt;f的行列式计算量不要过大&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;怎么找到f，f是一个带参数的函数，f的形式确定了，寻找f即寻找f的参数，可以找到一个使得q(y)和数据分布最一致的f（最大似然）.&lt;/p&gt;

&lt;h3 id=&quot;问题3&quot;&gt;问题3&lt;/h3&gt;
&lt;p&gt;x服从p(x)分布，若y=f2(f1(x)),则y服从什么分布？&lt;/p&gt;

&lt;p&gt;假设y服从q(y)分布&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;q(y) = p(g1(g2((y))) |det J(g1)|  |det J(g2)|  
g1=f1^-1
g2=f2^-1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;可以看出，通过一系列变换fi将x变为y，q(y)的形式非常简单。因此，对于问题2，我们可以寻找一系列满足可逆和行列式容易计算这两个条件的f，来构建非常复杂的变换（多层的神经网络）。
通过梯度方法可以计算出神经网络的参数。概率估计问题被转换为一个深度神经网络的学习问题。&lt;/p&gt;

&lt;h2 id=&quot;为flow选择f&quot;&gt;为flow选择f&lt;/h2&gt;

&lt;h3 id=&quot;nice和nvp&quot;&gt;NICE和NVP&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;一个很巧妙的构造，把x拆成两部分，第一部分不变，第二部分的进行缩放变换，缩放比例是一个关于第一部分的函数，函数可逆，且行列式的值非常容易求得。&lt;/li&gt;
  &lt;li&gt;为了使得每一维都可以被变换，每次拆分时选择不同的维度作为不变的第一部分。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;glow&quot;&gt;Glow&lt;/h3&gt;

&lt;p&gt;在NVP中需要每次拆分选择不同的维度，使用1X1的卷积可以直接混合不同维度之间的信息，GFlow通过在每个NVP的f之间假如一个1X1的卷积（或者一个前向层），可以不需要每次拆分选择不同的维度。&lt;/p&gt;

&lt;h2 id=&quot;自回归flow&quot;&gt;自回归flow&lt;/h2&gt;</content><author><name></name></author><summary type="html">预备知识 雅各比阵 J</summary></entry><entry><title type="html">语音合成Tacotron</title><link href="http://localhost:4000/tts/2019/02/15/tts-tacotron.html" rel="alternate" type="text/html" title="语音合成Tacotron" /><published>2019-02-15T11:12:00+08:00</published><updated>2019-02-15T11:12:00+08:00</updated><id>http://localhost:4000/tts/2019/02/15/tts-tacotron</id><content type="html" xml:base="http://localhost:4000/tts/2019/02/15/tts-tacotron.html">&lt;h1 id=&quot;tacotron系列论文笔记&quot;&gt;Tacotron系列论文笔记&lt;/h1&gt;

&lt;ul class=&quot;task-list&quot;&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;TACOTRON: TOWARDS END-TO-END SPEECH SYNTHESIS&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; checked=&quot;checked&quot; /&gt;Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;List item Uncovering Latent Style Factors for Expressive Speech Synthesis&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Predicting Expressive Speaking Style From Text in End-to-End Speech Synthesis&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Semi-Supervised Training for Improving Data Efficiency in End-to-End Speech Synthesis&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Hierarchical Generative Modeling for Controllable Speech Synthesis&lt;/li&gt;
  &lt;li class=&quot;task-list-item&quot;&gt;&lt;input type=&quot;checkbox&quot; class=&quot;task-list-item-checkbox&quot; disabled=&quot;disabled&quot; /&gt;Disentangling Correlated Speaker and Noise for Speech Synthesis via Data Augmentation and Adversarial Factorization&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;目标：通过学习Tacotron系列论文，
了解语音合成领域的研究方向，研究方法，专用术语，评估方法和当前的系统性能&lt;/p&gt;

&lt;h2 id=&quot;基本的e2e语音合成系统介绍&quot;&gt;基本的E2E语音合成系统介绍&lt;/h2&gt;

&lt;p&gt;参数TTS的系统结构为 音频特征生成 + 声码器。
参考WAVENET: A GENERATIVE MODEL FOR RAW AUDIO的附录&lt;/p&gt;

&lt;p&gt;其中，音频特征生成模块，输入是待合成的文本，输出为音频特征序列。
传统的声码器（vocoder)是基于语音信号处理方法，将频域特征还原回时域信号。2016年google提出的基于神经网络的声码器Wavenet，后来做了进一步优化，提升了合成音频质量并加快了计算速度，目前可以完全替代传统声码器。&lt;/p&gt;

&lt;p&gt;Google TTS组的工作 &lt;a href=&quot;https://google.github.io/tacotron/&quot;&gt;tacotron&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;17年初到18年底，Google发表了一系列利用神经网络进行音频特征生成的方法，极大的提升了合成的质量，工作包括&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;一种基于Encoder-Decoder神经网络结构的参数TTS中的声学模型，称之为Tacotron&lt;/li&gt;
  &lt;li&gt;利用神经网络建模风格（style）信息，允许指定style&lt;/li&gt;
  &lt;li&gt;利用文本中获得style信息&lt;/li&gt;
  &lt;li&gt;多说话人&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1tacotron开篇之作&quot;&gt;1.Tacotron开篇之作&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;TACOTRON: TOWARDS END-TO-END SPEECH SYNTHESIS&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;摘要&lt;/p&gt;

&lt;p&gt;A text-to-speech synthesis system typically consists of multiple stages,
such as a text analysis frontend, an acoustic model and an audio synthesis module.
Build-ing these components often requires extensive domain expertise and may contain brittle design choices. 
In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters.
Given &amp;lt;text, audio&amp;gt; pairs, the model can be trained completely from scratch with random initialization.
We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task.
Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English,
outperforming a production parametric system in terms of naturalness.
In addition, since Tacotron generates speech at the frame level, it’s substantially faster than sample-level autoregressive methods.&lt;/p&gt;

&lt;p&gt;专业术语&lt;/p&gt;

&lt;p&gt;parametric synthesis system 参数语音合成系统&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;text analysis frontend&lt;/li&gt;
  &lt;li&gt;an acoustic model&lt;/li&gt;
  &lt;li&gt;an audio synthesis module&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;mean opinion score - MOS TTS里的主观评测指标，有1-5分选项，专业测听人员进行打分&lt;/p&gt;

&lt;p&gt;frame-level 帧级别，对于8K采样的声音，如果帧移是10ms，每秒800帧。每帧根据帧长对应多个采样点，比如帧长25ms，8k的数据，每帧对应采样点个数为200个。
从帧级别的特征，可以生成采样点级别的信号。
samlple-level 采样点级别，对于8K采样的声音，每秒生成8000个采样点。&lt;/p&gt;

&lt;p&gt;Tacotron是Encoder-Decoder 结构的网络。&lt;/p&gt;

&lt;p&gt;Encoder = WordEmbeding+PreNet+CBHG&lt;/p&gt;

&lt;p&gt;CBHG&lt;/p&gt;

&lt;p&gt;Convolution Bank + Highway + bidirectional-GRU&lt;/p&gt;

&lt;p&gt;Convolution Bank 是不同width的1-D filters，每个宽度的bank包含一组filters，每一时间点上所有filter的输出拼接起来。
比如，如果输入序列维度是128*T，使用16个不同宽度的bank（宽度分别为1-16），若每种宽度的bank都有256个filter，stride=1，做好padding，则每个bank输出是256*T序列。
将所有bank拼接起来，最终输出是（256*16）* T的序列。&lt;/p&gt;

&lt;p&gt;对于宽度为k的filter，则一个filter的参数有k*128个.假设每宽度k有256个filter（256个输出channel）
总共的参数量为sum_k*128*256,若k取1-16，即一个CBHG层的参数量为17*8*128*256=4400000。
CBHG输出每一时刻的维度为256*16&lt;/p&gt;

&lt;p&gt;CBHG中还使用了res连接，通过Convolution Bank的序列，最后使用一个输出channel和CBHG的输入序列channel大小一样的卷积，使得卷积后维度和原始输入相同，从而可以相加。&lt;/p&gt;

&lt;p&gt;整个CBHG操作可以看作是序列的一种特征变化。原始的输入序列上每一个时刻点的特征仅是自身的信息，而通过CBank可以捕获不同窗长度内上下文时文本的信息，RNN也可以获得长距离上下文的信息，
从而经过变换后的特征序列，每个时刻上的特征出了包含当前时刻本身信息，还包含了当前时刻在此上下文情况下的信息。&lt;/p&gt;

&lt;p&gt;Decoder&lt;/p&gt;

&lt;p&gt;Decoder部分分两个网络&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;第一部分是PreNet+Attention RNN网络，该网络输出mel谱参数。&lt;/li&gt;
  &lt;li&gt;第二部分是一个CBHG网络，mel谱参数再经过该CBHG网络得到线性谱参数。&lt;/li&gt;
  &lt;li&gt;优化时同时使用这个两个输出和对应标注的误差作为优化目标。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;线性谱是原语音帧的完整频谱表示。
梅尔谱特征是一个变换到梅尔域并且加三角窗且降低分辨率的更低维表示。&lt;/p&gt;

&lt;p&gt;为了加速训练，Decoder中的PreNet+Attention RNN网络每个时刻输出连续多帧（比如3帧）的特征。&lt;/p&gt;

&lt;p&gt;假设每帧是40个输出，则使用一个128&lt;em&gt;120的全联接层， 其中128为GRU的输出维度，120 = 40&lt;/em&gt;3. 训练时&lt;/p&gt;

&lt;p&gt;对于输出多帧的谱参数，只把每个时间点输出的多帧中的最后一帧作为下一个时间的输入。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;In inference, at decoder step t, the last frame of the r predictions is fed as input to the decoder at step t + 1.
Note that feeding the last prediction is an ad-hoc choice here，we could use all r predictions. 
During training, we always feed every r-th ground truth frame to the decoder. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Decoder的第一时刻点的输入为一个全零向量。&lt;/p&gt;

&lt;p&gt;PreNet+Attention RNN直接输出线性谱,效果不好，论文中有图片比较通过PreNet+Attention RNN直接输出线性谱和
PreNet+Attention RNN输出mel谱然后再经过CBHG网络输出线性谱，后者看起来共振峰清晰很多。&lt;/p&gt;

&lt;p&gt;Attention Align的图
decoder中的输出和encoder中的输出做attention，因为TTS的输出和输入是同样的顺序（语音识别也是同序的，机器翻译是不同序的），
因此，因此我们期望attention align的权重图，对于每个输出都主要align在某个输入时刻点上，且是平滑变化的，反映到图上时应该是一个
清晰的从左到右上升的台阶形状。&lt;/p&gt;

&lt;p&gt;Tacotron的MOS值好于当时最好的参数模型。使用了Griffin-Lim的Vocoder，效果比当时最好的拼接系统略差。
Tacotron 3.82 ± 0.085&lt;br /&gt;
Parametric 3.69 ± 0.109
Concatenative 4.09 ± 0.119&lt;/p&gt;

&lt;h2 id=&quot;2tacotronwavenet&quot;&gt;2.Tacotron+Wavenet&lt;/h2&gt;
&lt;p&gt;Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions&lt;/p&gt;

&lt;p&gt;This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. 
The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, 
followed by a mod-ified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. 
Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. 
To validate our design choices, we present ablation studies of key components of our system and
evaluate the im-pact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features.
We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.&lt;/p&gt;

&lt;p&gt;此论文也被称为Tacotron2，其对Tacotron开篇之作的中的系统做了如下改进：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;简化了CBHG，去掉了其中的Highway网络&lt;/li&gt;
  &lt;li&gt;声学模型网络输出mel特征（而不是线性谱特征+F0）&lt;/li&gt;
  &lt;li&gt;vocoder从Griffin-Lim换做了Wavenet，&lt;/li&gt;
  &lt;li&gt;MOS直接到达4.53，逼近真人发音的4.58MOS值&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3带风格style的合成方法&quot;&gt;3.带风格Style的合成方法&lt;/h2&gt;

&lt;p&gt;Uncovering Latent Style Factors for Expressive Speech Synthesis&lt;/p&gt;

&lt;p&gt;Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron&lt;/p&gt;

&lt;p&gt;Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis&lt;/p&gt;

&lt;p&gt;Predicting Expressive Speaking Style From Text in End-to-End Speech Synthesis&lt;/p&gt;</content><author><name></name></author><summary type="html">Tacotron系列论文笔记</summary></entry><entry><title type="html">Attention/Transformer/Transformer XL</title><link href="http://localhost:4000/dl/2019/02/10/dl-transformer.html" rel="alternate" type="text/html" title="Attention/Transformer/Transformer XL" /><published>2019-02-10T10:00:00+08:00</published><updated>2019-02-10T10:00:00+08:00</updated><id>http://localhost:4000/dl/2019/02/10/dl-transformer</id><content type="html" xml:base="http://localhost:4000/dl/2019/02/10/dl-transformer.html">&lt;h1 id=&quot;attentiontransformertransformer-xl&quot;&gt;Attention/Transformer/Transformer XL&lt;/h1&gt;

&lt;h2 id=&quot;transformer&quot;&gt;Transformer&lt;/h2&gt;

&lt;p&gt;Attention&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Attention可以抽象为一个Q（query）和一组[(K，V)]（key，value).&lt;/li&gt;
  &lt;li&gt;给定一个Q，根据Q和K的相关程度（attention度量方法），选择K对应的V。&lt;/li&gt;
  &lt;li&gt;和数据库检索不同，并不是只使用相关度最大的K对应的V，而是每个V都使用。通过度量公式计算Q和K的相关程度，和Q越相关的K其对应的V的权重越大，将所有的V加权求和。&lt;/li&gt;
  &lt;li&gt;Q，K，V都是向量。且很多结构里使用的Attention机制，V就是K本身。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Multihead Attention&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Multihead是指，同时使用多组attention进行计算。&lt;/li&gt;
  &lt;li&gt;因为输入都是Q，[(K，V)].如果直接用Q，K，V来计算，每组attention机制算出的结果是一样的。&lt;/li&gt;
  &lt;li&gt;每组attention将K，V，Q分别做线性变化（线性变换的参数是网络的参数一部分），相当于计算信息在某个线性子空间上的attention，从而每组得到的attention结果是不同的。各组计算的结果再拼接起来作为最终结果。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Inter-Attention&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;在attention最初应用于NN时，Q是decoder中的hidden输出，K，V是encoder中的hidden输出（且K和V一样）.因此Q和K实际是两个不同序列上的信息，我称之为Inter-Attention，或者External-Attention。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Self-Attention&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;是指attention不跨越decoder和encoder，在encoder序列内或者decoder序列内进行。也可以叫Intra-Attention，Internal-Attental。&lt;/li&gt;
  &lt;li&gt;比如encoder中序列的每一时刻的某层信息（比如文本的embedding）都会和其他各时刻做attention操作，从而每个时刻都可以直接捕获其他时刻的信息。&lt;/li&gt;
  &lt;li&gt;在Self-attention中，Q和K是属于同一个序列的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Positional Encoding（PE）&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;self-attention不像RNN或者CNN可以捕捉到序列的顺序性，因此当顺序是重要的时候，可以通过Positional Encoding（PE）来为数据加入位置相关的信息。可以使用正余弦函数来构建，每一维选择不同的角频率，给定位置则PE值是确定的，且不同位置的PE值一定不一样。PE和输入直接相加。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Masked Multihead Self-Attention&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Decoder-Block里有个Masked Multihead Self-Attention中的Masked是指做attention时只和当前时刻之前的时刻做attention。因为decoder时输出（也是下一时刻的输入）从左往右依次产生的，在t时刻时，后面的数据还未产生。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Transformer的整体结构&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Transformer = Encoder + Decoder&lt;/li&gt;
  &lt;li&gt;Encoder=Positional encoding + [Encoder-Block] * N&lt;/li&gt;
  &lt;li&gt;Encoder-Block=[Res+Multihead Self-Attention] + [Res+Dense]&lt;/li&gt;
  &lt;li&gt;Decoder=Positional encoding + [Decoder-Block] * N&lt;/li&gt;
  &lt;li&gt;Decoder-Block=[Res+Masked Multihead Self-Attention] + [Res+Multihead Inter-Attention] + [Res+Dense]&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;transformer的应用&quot;&gt;Transformer的应用&lt;/h2&gt;
&lt;h3 id=&quot;各种端到端的序列转换任务&quot;&gt;各种端到端的序列转换任务&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;自然语言处理
    &lt;ul&gt;
      &lt;li&gt;开山论文即展示在机器翻译和Constituency Parsing上应用达到state-of-the-art。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;语音识别：
    &lt;ul&gt;
      &lt;li&gt;self-attention比较适合自然语言处理任务中希望捕获长距离上下文影响的情况。语音识别输入是帧级别的声学特征，在encoder阶段self-attention意义不大。decode阶段，目前的E2E识别系统建模单元一般都没到词级别（中文是音节/音素，字母类语言用音素/字符/音节/子词），也没必要使用覆盖特别长的上下文的self-attention。&lt;/li&gt;
      &lt;li&gt;不过也有学者尝试用Transformer来做识别，Speech-transformer:A no-recurrence sequence-to-sequence model for speech recognition&lt;/li&gt;
      &lt;li&gt;pytorch代码实现 https://github.com/kaituoxu/Speech-Transformer&lt;/li&gt;
      &lt;li&gt;大部分语音识别场景要求streaming，直接用encoder-decoder架构不适合。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;语音合成的输入是文本，可以考虑应用self-attention捕获长距离上下文。
    &lt;ul&gt;
      &lt;li&gt;微软的文章 https://arxiv.org/pdf/1809.08895.pdf&lt;/li&gt;
      &lt;li&gt;语音合成不像nlp的任务需要关注特别长的上下文，tacotron中的conv bank本身覆盖的上下文长度（从1到16）已经足够长了。感觉使用self-attention还是conv bank都差不多。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bert&quot;&gt;BERT&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;OpenAI的Generative Pre-trained Transformer（GPT）最早Transformer来进行LM训练，该模型从左往右预测下一个词，类似于Transformer的Decoder部分，只使用了词的单侧上下文。&lt;/li&gt;
  &lt;li&gt;BERT希望使用self-attention且能同时利用左右的上下文，但是直接用双侧的self-attention会使得在预测下一个词的时，输入信息中直接能看到下一个词是什么，模型无法得到有效学习。为了解决这个问题引入了masked LM的训练方法。对于要预测的词，将输入序列中对应的词置为一个特殊字符’[masked]’, 这样输入就看不到待预测的词的信息了。&lt;/li&gt;
  &lt;li&gt;BERT还增加了一个预测一个句子是不是另一个句子的下一个句子的任务来训练模型（是不是这个标注可以直接从语料免费获取，不需要额外标注）。将两个句子拼起来，用’[sep]’特殊字符分割，然后句首增加’[cls]’特殊字符，用’[cls]’的输出经过一个前向分类网络作为预测结果。&lt;/li&gt;
  &lt;li&gt;网络参数配置:
    &lt;ul&gt;
      &lt;li&gt;BERT-Base :12-layer+ 12-heads + 768-hidden，110M parameters&lt;/li&gt;
      &lt;li&gt;BERT-Large : 24-layer, 1024-hidden, 16-heads, 340M parameters&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;语言模型天生不需要人工标注，有海量训练数据。训练出的模型可作为其他任务的特征提取模块。将原文本序列转换成一个等长新序列。在几乎所有任务上都得到了很大的提升。&lt;/li&gt;
  &lt;li&gt;和word2vec直接为每个词提供一个表示（查表，不需要计算时间）不同，BERT是将每个词根据其所在上下完将其转为一个向量表示（称为Contextualized word-embeddings），优点是更好的利用了词当前所处上下文，缺点是需要在线计算且计算量不小。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;transformer的优化&quot;&gt;Transformer的优化&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Transformer XL&lt;/li&gt;
  &lt;li&gt;Evoled Transformer&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;资料&quot;&gt;资料&lt;/h2&gt;
&lt;h3 id=&quot;attention&quot;&gt;Attention&lt;/h3&gt;
&lt;p&gt;explain&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Attention/NTM/Transformer/SNAIL等 https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;survey&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;https://arxiv.org/pdf/1811.05544.pdf&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;transformer-1&quot;&gt;Transformer&lt;/h3&gt;
&lt;p&gt;paper&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;https://arxiv.org/pdf/1706.03762.pdf&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;explain&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;傻瓜版图文讲解 http://jalammar.github.io/illustrated-transformer/&lt;/li&gt;
  &lt;li&gt;论文总结1 https://hub.packtpub.com/paper-in-two-minutes-attention-is-all-you-need/&lt;/li&gt;
  &lt;li&gt;论文总结2 http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/&lt;/li&gt;
  &lt;li&gt;论文讲解版带pytorch代码 http://nlp.seas.harvard.edu/2018/04/03/attention.html#model-architecture&lt;/li&gt;
  &lt;li&gt;Google官方报告 https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;video and slides&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;https://www.youtube.com/watch?v=rBCqOTEfxvg&lt;/li&gt;
  &lt;li&gt;https://drive.google.com/file/d/0B8BcJC1Y8XqobGNBYVpteDdFOWc/view&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bert-1&quot;&gt;BERT&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Google官方代码 https://github.com/google-research/bert&lt;/li&gt;
  &lt;li&gt;图文并茂，还顺带解释了一些列预训练表示模型 http://jalammar.github.io/illustrated-bert/&lt;/li&gt;
  &lt;li&gt;原始论文 https://arxiv.org/pdf/1810.04805.pdf&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;transformer-xl&quot;&gt;Transformer XL&lt;/h3&gt;
&lt;p&gt;paper&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;https://arxiv.org/pdf/1901.02860.pdf&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;code&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;https://github.com/kimiyoung/transformer-xl&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Attention/Transformer/Transformer XL</summary></entry><entry><title type="html">Jekyll Example</title><link href="http://localhost:4000/jekyll/update/2019/01/01/welcome-to-jekyll.html" rel="alternate" type="text/html" title="Jekyll Example" /><published>2019-01-01T11:11:59+08:00</published><updated>2019-01-01T11:11:59+08:00</updated><id>http://localhost:4000/jekyll/update/2019/01/01/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2019/01/01/welcome-to-jekyll.html">&lt;p&gt;You’ll find this post in your &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code class=&quot;highlighter-rouge&quot;&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Tom'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints 'Hi, Tom' to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.</summary></entry></feed>